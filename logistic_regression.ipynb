{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression  (Vicent Liu)\n",
    "\n",
    "Logistic regression is the classification counterpart to linear regression. Predictions are mapped to be between 0 and 1 through the logistic function, which means that predictions can be interpreted as class probabilities.\n",
    "The models themselves are still \"linear,\" so they work well when your classes are linearly separable (i.e. they can be separated by a single decision surface). Logistic regression can also be regularized by penalizing coefficients with a tunable penalty strength.\n",
    "\n",
    "Strengths: Outputs have a nice probabilistic interpretation, and the algorithm can be regularized to avoid overfitting. \n",
    "    Logistic models can be updated easily with new data using stochastic gradient descent. \n",
    "    \n",
    "Weaknesses: Logistic regression tends to underperform when there are multiple or non-linear decision boundaries. They are not flexible enough to naturally capture more complex relationships.\n",
    "\n",
    "Reference: http://scikit-learn.org/stable/modules/linear_model.html#logistic-regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import _pickle as cPickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "from sklearn.cross_validation import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=10000.0, class_weight=None, dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
       "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./data/ny_hmda_2015_minmax.csv')\n",
    "\n",
    "x = np.array(df.drop(['action_taken'],1)) \n",
    "y = np.array(df['action_taken'])\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.2)\n",
    "\n",
    "logreg = linear_model.LogisticRegression(C=1e4)\n",
    "logreg.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82411292892807819"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Z = logreg.predict(X)\n",
    "logreg.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.824019972379\n",
      "0.82419260597\n",
      "0.824112928928\n",
      "0.824112928928\n",
      "0.824112928928\n",
      "0.824099649421\n",
      "0.824126208435\n",
      "0.824099649421\n",
      "0.824112928928\n",
      "0.824112928928\n"
     ]
    }
   ],
   "source": [
    "#test different C\n",
    "for i in range(10):\n",
    "    logreg = linear_model.LogisticRegression(C=10**i)\n",
    "    print(logreg.fit(x_train, y_train).score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.824046531393\n",
      "0.824152767449\n",
      "0.824152767449\n",
      "0.824139487942\n",
      "0.824152767449\n",
      "0.82419260597\n",
      "0.824152767449\n",
      "0.824166046956\n",
      "0.824166046956\n",
      "0.824166046956\n"
     ]
    }
   ],
   "source": [
    "#test penalty = l1\n",
    "for i in range(10):\n",
    "    logreg = linear_model.LogisticRegression(C=10**i, penalty='l1')\n",
    "    print(logreg.fit(x_train, y_train).score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.819743971104\n"
     ]
    }
   ],
   "source": [
    "#TEST NORM DATASET\n",
    "df_norm = pd.read_csv('./data/ny_hmda_2015_normalize.csv')\n",
    "\n",
    "x_norm = np.array(df_norm.drop(['action_taken'],1)) \n",
    "y_norm = np.array(df_norm['action_taken'])\n",
    "x_train_norm, x_test_norm, y_train_norm, y_test_norm = train_test_split(x_norm,y_norm,test_size = 0.2)\n",
    "\n",
    "logreg = linear_model.LogisticRegression()\n",
    "print(logreg.fit(x_train_norm, y_train_norm).score(x_test_norm, y_test_norm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.823329438011\n"
     ]
    }
   ],
   "source": [
    "#TEST ROBUST DATASET\n",
    "df_robust = pd.read_csv('./data/ny_hmda_2015_robust.csv')\n",
    "\n",
    "x_robust = np.array(df_robust.drop(['action_taken'],1)) \n",
    "y_robust = np.array(df_robust['action_taken'])\n",
    "x_train_robust, x_test_robust, y_train_robust, y_test_robust = train_test_split(x_robust,y_robust,test_size = 0.2)\n",
    "\n",
    "logreg = linear_model.LogisticRegression()\n",
    "print(logreg.fit(x_train_robust, y_train_robust).score(x_test_robust, y_test_robust))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.81326357  0.83097843  0.85190694  0.84667481  0.81884096  0.81517582\n",
      "  0.82999575  0.8089825   0.68816233  0.88937583]\n"
     ]
    }
   ],
   "source": [
    "#Cross Validation\n",
    "scores = cross_val_score(linear_model.LogisticRegression(), x_robust, y_robust, scoring='accuracy', cv=10)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cPickle.dump(logreg,open('models/logreg_model.p', 'wb'))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00730146,  2.        ,  1.        ,  1.        ,  2.        ,\n",
       "        1.        ,  3.        ,  1.        ,  7.        ,  1.        ,\n",
       "        0.00728592,  0.28884682,  0.29711717,  0.53991365,  0.20210518,\n",
       "        0.28043419,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        1.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  1.        ,\n",
       "        0.        ,  0.        ])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
