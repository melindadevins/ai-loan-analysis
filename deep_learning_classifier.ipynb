{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning Classifier (Melinda Xiao-Devins)\n",
    "\n",
    "Implement deep learning modle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import model_from_json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.1.0\n",
      "Default GPU Device: /gpu:0\n"
     ]
    }
   ],
   "source": [
    "from distutils.version import LooseVersion\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "#from tensorflow.python.layers.core import Dense\n",
    "# Check TensorFlow Version\n",
    "assert LooseVersion(tf.__version__) >= LooseVersion('1.1'), 'Please use TensorFlow version 1.1 or newer'\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "# Check for a GPU\n",
    "if not tf.test.gpu_device_name():\n",
    " warnings.warn('No GPU found. Please use a GPU to train your neural network.')\n",
    "else:\n",
    " print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of records: 376516\n",
      "Toatl numver of features: 38\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "#df = pd.read_csv('./data/ny_hmda_2015_normalize.csv', low_memory=False, header=0, delimiter=\",\")\n",
    "df = pd.read_csv('./data/ny_hmda_2015_robust.csv', low_memory=False, header=0, delimiter=\",\")\n",
    "\n",
    "#print(dataframe.loc[:,:])\n",
    "num_rows = df.shape[0]\n",
    "num_col = df.shape[1]\n",
    "print (\"Total number of records: {}\".format(num_rows))\n",
    "print (\"Toatl numver of features: {}\".format(num_col))\n",
    "\n",
    "X = np.array(df.drop(['action_taken'],1)) \n",
    "Y = np.array(df['action_taken'])\n",
    "\n",
    "#Split into train and test set\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a fully connected network with 4 layers\n",
    "model = Sequential()\n",
    "\n",
    "#input layer, it has 256 neurons, it must have right number of inputs, which is the number of features\n",
    "model.add(Dense(256, input_dim=num_col-1, kernel_initializer='normal', activation='relu'))\n",
    "\n",
    "# hideen layer has 128 neurons\n",
    "model.add(Dense(128, kernel_initializer='normal', activation='relu'))\n",
    "\n",
    "# hideen layer has 64 neurons\n",
    "model.add(Dense(128, kernel_initializer='normal', activation='relu'))\n",
    "\n",
    "# hideen layer has 64 neurons\n",
    "model.add(Dense(64, kernel_initializer='normal', activation='relu'))\n",
    "\n",
    "# output layer has 1 neuron to predict\n",
    "# Use sigmoid for output layer activation function to ensure network output is bw. 0 and 1\n",
    "model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Compile model\n",
    "\n",
    "# loss function: logarithmic loss, which is binary_crossentropy for binary classification\n",
    "# use 'adam' optimizer for gradient descent algorithm \n",
    "# collect accuracy during training\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "301212/301212 [==============================] - 14s 46us/step - loss: 0.3389 - acc: 0.8264\n",
      "Epoch 2/100\n",
      "301212/301212 [==============================] - 13s 43us/step - loss: 0.3285 - acc: 0.8342\n",
      "Epoch 3/100\n",
      "301212/301212 [==============================] - 13s 44us/step - loss: 0.3265 - acc: 0.8353\n",
      "Epoch 4/100\n",
      "301212/301212 [==============================] - 13s 43us/step - loss: 0.3249 - acc: 0.8365\n",
      "Epoch 5/100\n",
      "301212/301212 [==============================] - 13s 43us/step - loss: 0.3238 - acc: 0.8372\n",
      "Epoch 6/100\n",
      "301212/301212 [==============================] - 13s 43us/step - loss: 0.3232 - acc: 0.8377\n",
      "Epoch 7/100\n",
      "301212/301212 [==============================] - 13s 43us/step - loss: 0.3225 - acc: 0.8380\n",
      "Epoch 8/100\n",
      "301212/301212 [==============================] - 13s 43us/step - loss: 0.3219 - acc: 0.8386\n",
      "Epoch 9/100\n",
      "301212/301212 [==============================] - 13s 44us/step - loss: 0.3215 - acc: 0.8387\n",
      "Epoch 10/100\n",
      "301212/301212 [==============================] - 13s 43us/step - loss: 0.3211 - acc: 0.8391\n",
      "Epoch 11/100\n",
      "301212/301212 [==============================] - 13s 44us/step - loss: 0.3207 - acc: 0.8394\n",
      "Epoch 12/100\n",
      "301212/301212 [==============================] - 13s 44us/step - loss: 0.3201 - acc: 0.8395\n",
      "Epoch 13/100\n",
      "301212/301212 [==============================] - 13s 43us/step - loss: 0.3197 - acc: 0.8400\n",
      "Epoch 14/100\n",
      "301212/301212 [==============================] - 13s 43us/step - loss: 0.3193 - acc: 0.8408\n",
      "Epoch 15/100\n",
      "301212/301212 [==============================] - 13s 43us/step - loss: 0.3190 - acc: 0.8407\n",
      "Epoch 16/100\n",
      "301212/301212 [==============================] - 13s 43us/step - loss: 0.3187 - acc: 0.8408\n",
      "Epoch 17/100\n",
      "301212/301212 [==============================] - 13s 44us/step - loss: 0.3184 - acc: 0.8409\n",
      "Epoch 18/100\n",
      "301212/301212 [==============================] - 13s 43us/step - loss: 0.3180 - acc: 0.8414\n",
      "Epoch 19/100\n",
      "301212/301212 [==============================] - 13s 43us/step - loss: 0.3178 - acc: 0.8418\n",
      "Epoch 20/100\n",
      "301212/301212 [==============================] - 13s 43us/step - loss: 0.3175 - acc: 0.8417\n",
      "Epoch 21/100\n",
      "301212/301212 [==============================] - 13s 43us/step - loss: 0.3172 - acc: 0.8420\n",
      "Epoch 22/100\n",
      "301212/301212 [==============================] - 13s 43us/step - loss: 0.3169 - acc: 0.8423\n",
      "Epoch 23/100\n",
      "301212/301212 [==============================] - 13s 43us/step - loss: 0.3165 - acc: 0.8424\n",
      "Epoch 24/100\n",
      "301212/301212 [==============================] - 13s 43us/step - loss: 0.3162 - acc: 0.8428\n",
      "Epoch 25/100\n",
      "301212/301212 [==============================] - 13s 43us/step - loss: 0.3160 - acc: 0.8430\n",
      "Epoch 26/100\n",
      "301212/301212 [==============================] - 13s 43us/step - loss: 0.3156 - acc: 0.8434\n",
      "Epoch 27/100\n",
      "301212/301212 [==============================] - 13s 44us/step - loss: 0.3153 - acc: 0.8430\n",
      "Epoch 28/100\n",
      "301212/301212 [==============================] - 13s 43us/step - loss: 0.3151 - acc: 0.8432\n",
      "Epoch 29/100\n",
      "301212/301212 [==============================] - 13s 43us/step - loss: 0.3147 - acc: 0.8436\n",
      "Epoch 30/100\n",
      "301212/301212 [==============================] - 13s 43us/step - loss: 0.3146 - acc: 0.8440\n",
      "Epoch 31/100\n",
      "301212/301212 [==============================] - 13s 43us/step - loss: 0.3142 - acc: 0.8440\n",
      "Epoch 32/100\n",
      "301212/301212 [==============================] - 13s 43us/step - loss: 0.3138 - acc: 0.8442\n",
      "Epoch 33/100\n",
      "301212/301212 [==============================] - 13s 43us/step - loss: 0.3137 - acc: 0.8443\n",
      "Epoch 34/100\n",
      "301212/301212 [==============================] - 13s 43us/step - loss: 0.3135 - acc: 0.8447\n",
      "Epoch 35/100\n",
      "301212/301212 [==============================] - 13s 44us/step - loss: 0.3133 - acc: 0.8446\n",
      "Epoch 36/100\n",
      "301212/301212 [==============================] - 13s 44us/step - loss: 0.3129 - acc: 0.8451\n",
      "Epoch 37/100\n",
      "301212/301212 [==============================] - 13s 43us/step - loss: 0.3126 - acc: 0.8454\n",
      "Epoch 38/100\n",
      "301212/301212 [==============================] - 13s 43us/step - loss: 0.3124 - acc: 0.8454\n",
      "Epoch 39/100\n",
      "301212/301212 [==============================] - 13s 43us/step - loss: 0.3123 - acc: 0.8451\n",
      "Epoch 40/100\n",
      "301212/301212 [==============================] - 13s 43us/step - loss: 0.3119 - acc: 0.8455\n",
      "Epoch 41/100\n",
      "301212/301212 [==============================] - 13s 43us/step - loss: 0.3117 - acc: 0.8458\n",
      "Epoch 42/100\n",
      "301212/301212 [==============================] - 13s 43us/step - loss: 0.3114 - acc: 0.8459\n",
      "Epoch 43/100\n",
      "301212/301212 [==============================] - 13s 43us/step - loss: 0.3112 - acc: 0.8460\n",
      "Epoch 44/100\n",
      "301212/301212 [==============================] - 13s 43us/step - loss: 0.3109 - acc: 0.8464\n",
      "Epoch 45/100\n",
      "301212/301212 [==============================] - 13s 43us/step - loss: 0.3105 - acc: 0.8467\n",
      "Epoch 46/100\n",
      "301212/301212 [==============================] - 13s 44us/step - loss: 0.3103 - acc: 0.8466\n",
      "Epoch 47/100\n",
      "301212/301212 [==============================] - 13s 44us/step - loss: 0.3104 - acc: 0.8467\n",
      "Epoch 48/100\n",
      "301212/301212 [==============================] - 13s 43us/step - loss: 0.3098 - acc: 0.8468\n",
      "Epoch 49/100\n",
      "301212/301212 [==============================] - 13s 44us/step - loss: 0.3097 - acc: 0.8471\n",
      "Epoch 50/100\n",
      "301212/301212 [==============================] - 13s 44us/step - loss: 0.3090 - acc: 0.8475\n",
      "Epoch 51/100\n",
      "301212/301212 [==============================] - 13s 44us/step - loss: 0.3089 - acc: 0.8473\n",
      "Epoch 52/100\n",
      "301212/301212 [==============================] - 14s 45us/step - loss: 0.3087 - acc: 0.8476\n",
      "Epoch 53/100\n",
      "301212/301212 [==============================] - 13s 44us/step - loss: 0.3083 - acc: 0.8482\n",
      "Epoch 54/100\n",
      "301212/301212 [==============================] - 13s 44us/step - loss: 0.3083 - acc: 0.8479\n",
      "Epoch 55/100\n",
      "301212/301212 [==============================] - 14s 47us/step - loss: 0.3078 - acc: 0.8485\n",
      "Epoch 56/100\n",
      "301212/301212 [==============================] - 31s 102us/step - loss: 0.3077 - acc: 0.8485\n",
      "Epoch 57/100\n",
      "301212/301212 [==============================] - 19s 63us/step - loss: 0.3073 - acc: 0.8491\n",
      "Epoch 58/100\n",
      "301212/301212 [==============================] - 14s 48us/step - loss: 0.3072 - acc: 0.8486\n",
      "Epoch 59/100\n",
      "301212/301212 [==============================] - 14s 48us/step - loss: 0.3068 - acc: 0.8492\n",
      "Epoch 60/100\n",
      "301212/301212 [==============================] - 14s 47us/step - loss: 0.3064 - acc: 0.8494\n",
      "Epoch 61/100\n",
      "301212/301212 [==============================] - 14s 48us/step - loss: 0.3065 - acc: 0.8495\n",
      "Epoch 62/100\n",
      "301212/301212 [==============================] - 14s 47us/step - loss: 0.3066 - acc: 0.8491\n",
      "Epoch 63/100\n",
      "301212/301212 [==============================] - 19s 64us/step - loss: 0.3057 - acc: 0.8495\n",
      "Epoch 64/100\n",
      "301212/301212 [==============================] - 18s 59us/step - loss: 0.3056 - acc: 0.8498\n",
      "Epoch 65/100\n",
      "301212/301212 [==============================] - 22s 74us/step - loss: 0.3052 - acc: 0.8499\n",
      "Epoch 66/100\n",
      "301212/301212 [==============================] - 23s 75us/step - loss: 0.3050 - acc: 0.8504\n",
      "Epoch 67/100\n",
      "301212/301212 [==============================] - 20s 66us/step - loss: 0.3048 - acc: 0.8505\n",
      "Epoch 68/100\n",
      "301212/301212 [==============================] - 18s 61us/step - loss: 0.3045 - acc: 0.8506\n",
      "Epoch 69/100\n",
      "301212/301212 [==============================] - 21s 69us/step - loss: 0.3040 - acc: 0.8508\n",
      "Epoch 70/100\n",
      "301212/301212 [==============================] - 19s 63us/step - loss: 0.3041 - acc: 0.8507\n",
      "Epoch 71/100\n",
      "301212/301212 [==============================] - 17s 56us/step - loss: 0.3040 - acc: 0.8507\n",
      "Epoch 72/100\n",
      "301212/301212 [==============================] - 13s 44us/step - loss: 0.3034 - acc: 0.8509\n",
      "Epoch 73/100\n",
      "301212/301212 [==============================] - 13s 44us/step - loss: 0.3032 - acc: 0.8514\n",
      "Epoch 74/100\n",
      "301212/301212 [==============================] - 13s 44us/step - loss: 0.3033 - acc: 0.8514\n",
      "Epoch 75/100\n",
      "301212/301212 [==============================] - 13s 44us/step - loss: 0.3029 - acc: 0.8513\n",
      "Epoch 76/100\n",
      "301212/301212 [==============================] - 13s 43us/step - loss: 0.3024 - acc: 0.8518\n",
      "Epoch 77/100\n",
      "301212/301212 [==============================] - 13s 44us/step - loss: 0.3022 - acc: 0.8516\n",
      "Epoch 78/100\n",
      "301212/301212 [==============================] - 13s 44us/step - loss: 0.3022 - acc: 0.8519\n",
      "Epoch 79/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "301212/301212 [==============================] - 13s 43us/step - loss: 0.3014 - acc: 0.8525\n",
      "Epoch 80/100\n",
      "301212/301212 [==============================] - 13s 44us/step - loss: 0.3015 - acc: 0.8522\n",
      "Epoch 81/100\n",
      "301212/301212 [==============================] - 13s 44us/step - loss: 0.3014 - acc: 0.8522\n",
      "Epoch 82/100\n",
      "301212/301212 [==============================] - 13s 44us/step - loss: 0.3011 - acc: 0.8526\n",
      "Epoch 83/100\n",
      "301212/301212 [==============================] - 13s 44us/step - loss: 0.3013 - acc: 0.8525\n",
      "Epoch 84/100\n",
      "301212/301212 [==============================] - 13s 44us/step - loss: 0.3006 - acc: 0.8526\n",
      "Epoch 85/100\n",
      "301212/301212 [==============================] - 13s 44us/step - loss: 0.3003 - acc: 0.8533\n",
      "Epoch 86/100\n",
      "301212/301212 [==============================] - 13s 44us/step - loss: 0.3002 - acc: 0.8535\n",
      "Epoch 87/100\n",
      "301212/301212 [==============================] - 13s 44us/step - loss: 0.2999 - acc: 0.8535\n",
      "Epoch 88/100\n",
      "301212/301212 [==============================] - 13s 44us/step - loss: 0.2998 - acc: 0.8533\n",
      "Epoch 89/100\n",
      "301212/301212 [==============================] - 13s 44us/step - loss: 0.2993 - acc: 0.8537\n",
      "Epoch 90/100\n",
      "301212/301212 [==============================] - 13s 43us/step - loss: 0.2992 - acc: 0.8538\n",
      "Epoch 91/100\n",
      "301212/301212 [==============================] - 13s 44us/step - loss: 0.2992 - acc: 0.8539\n",
      "Epoch 92/100\n",
      "301212/301212 [==============================] - 13s 44us/step - loss: 0.2987 - acc: 0.8537\n",
      "Epoch 93/100\n",
      "301212/301212 [==============================] - 13s 44us/step - loss: 0.2983 - acc: 0.8539\n",
      "Epoch 94/100\n",
      "301212/301212 [==============================] - 13s 44us/step - loss: 0.2982 - acc: 0.8541\n",
      "Epoch 95/100\n",
      "301212/301212 [==============================] - 13s 43us/step - loss: 0.2978 - acc: 0.8541\n",
      "Epoch 96/100\n",
      "301212/301212 [==============================] - 13s 43us/step - loss: 0.2976 - acc: 0.8548\n",
      "Epoch 97/100\n",
      "301212/301212 [==============================] - 13s 44us/step - loss: 0.2977 - acc: 0.8545\n",
      "Epoch 98/100\n",
      "301212/301212 [==============================] - 13s 44us/step - loss: 0.2976 - acc: 0.8544\n",
      "Epoch 99/100\n",
      "301212/301212 [==============================] - 13s 44us/step - loss: 0.2973 - acc: 0.8546\n",
      "Epoch 100/100\n",
      "301212/301212 [==============================] - 13s 43us/step - loss: 0.2969 - acc: 0.8551\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b464eb8eb8>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "# epochs: a fixed number of iterations through the dataset\n",
    "# batch size: the number of instances that are evaluated before a weight update in the network is performed \n",
    "model.fit(X_train, Y_train, epochs = 500, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "scores = model.evaluate(X_test, Y_test)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate predictions\n",
    "predictions = model.predict(X)\n",
    "print(\"the prediction bw. 0 and 1\")\n",
    "print(predictions)\n",
    "\n",
    "# round predictions\n",
    "rounded = [round(x[0]) for x in predictions]\n",
    "print(\"the rounded prediction\")\n",
    "print(rounded[0:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "precision, recall, fscore, support = score(Y_test, [round(x[0]) for x in model.predict(X_test)],average=\"macro\")\n",
    "print(\"precision={}, recall={}, fscore={}, support={}\".format(precision, recall, fscore, support))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Save Trained Model\n",
    "Save the trained model to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"model/model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model/model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Load Saved Model\n",
    "Load the saved model, and used it. It saves training time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load json and create model\n",
    "json_file = open('model/model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model/model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    " \n",
    "print(\"Use loaded model to predict\")\n",
    "# evaluate loaded model on test data\n",
    "loaded_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# calculate predictions\n",
    "predictions = model.predict(X)\n",
    "print(\"the prediction bw. 0 and 1\")\n",
    "print(predictions)\n",
    "\n",
    "# round predictions\n",
    "rounded = [round(x[0]) for x in predictions]\n",
    "print(\"the rounded prediction\")\n",
    "print(rounded[0:10])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = loaded_model.evaluate(X, Y, verbose=0)\n",
    "print(\"From saved model,  %s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "precision, recall, fscore, support = score(Y_test, [round(x[0]) for x in model.predict(X_test)],average=\"macro\")\n",
    "print(\"precision={}, recall={}, fscore={}, support={}\".format(precision, recall, fscore, support))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
