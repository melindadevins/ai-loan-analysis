{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning Classifier \n",
    "\n",
    "### Melinda Xiao-Devins\n",
    "\n",
    "Implement deep learning neural network classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/anaconda/envs/em_hack_mac/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import model_from_json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/em_hack_mac/lib/python3.6/site-packages/ipykernel_launcher.py:10: UserWarning: No GPU found. Please use a GPU to train your neural network.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "from distutils.version import LooseVersion\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "#from tensorflow.python.layers.core import Dense\n",
    "# Check TensorFlow Version\n",
    "assert LooseVersion(tf.__version__) >= LooseVersion('1.1'), 'Please use TensorFlow version 1.1 or newer'\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "# Check for a GPU\n",
    "if not tf.test.gpu_device_name():\n",
    " warnings.warn('No GPU found. Please use a GPU to train your neural network.')\n",
    "else:\n",
    " print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of records: 376516\n",
      "Toatl numver of features: 61\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "#df = pd.read_csv('./data/ny_hmda_2015_normalize.csv', low_memory=False, header=0, delimiter=\",\")\n",
    "df = pd.read_csv('./data/ny_hmda_2015_normalize.csv', low_memory=False, header=0, delimiter=\",\")\n",
    "\n",
    "#print(dataframe.loc[:,:])\n",
    "num_rows = df.shape[0]\n",
    "num_col = df.shape[1]\n",
    "print (\"Total number of records: {}\".format(num_rows))\n",
    "print (\"Toatl numver of features: {}\".format(num_col))\n",
    "\n",
    "X = np.array(df.drop(['action_taken'],1)) \n",
    "Y = np.array(df['action_taken'])\n",
    "\n",
    "#Split into train and test set\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a fully connected network with 4 layers\n",
    "model = Sequential()\n",
    "\n",
    "#input layer, it has 60 neurons, it must have right number of inputs, which is the number of features\n",
    "model.add(Dense(60, input_dim=num_col-1, kernel_initializer='normal', activation='relu'))\n",
    "\n",
    "# hideen layer has 32 neurons\n",
    "model.add(Dense(32, kernel_initializer='normal', activation='relu'))\n",
    "\n",
    "# hideen layer has 16 neurons\n",
    "model.add(Dense(16, kernel_initializer='normal', activation='relu'))\n",
    "\n",
    "# hideen layer has 8 neurons\n",
    "model.add(Dense(8, kernel_initializer='normal', activation='relu'))\n",
    "\n",
    "# output layer has 1 neuron to predict\n",
    "# Use sigmoid for output layer activation function to ensure network output is bw. 0 and 1\n",
    "model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Compile model\n",
    "\n",
    "# loss function: logarithmic loss, which is binary_crossentropy for binary classification\n",
    "# use 'adam' optimizer for gradient descent algorithm \n",
    "# collect accuracy during training\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "301212/301212 [==============================] - 5s 17us/step - loss: 0.3476 - acc: 0.8202\n",
      "Epoch 2/10\n",
      "301212/301212 [==============================] - 5s 15us/step - loss: 0.3318 - acc: 0.8310\n",
      "Epoch 3/10\n",
      "301212/301212 [==============================] - 5s 15us/step - loss: 0.3299 - acc: 0.8320\n",
      "Epoch 4/10\n",
      "301212/301212 [==============================] - 5s 16us/step - loss: 0.3290 - acc: 0.8331\n",
      "Epoch 5/10\n",
      "301212/301212 [==============================] - 5s 16us/step - loss: 0.3281 - acc: 0.8330\n",
      "Epoch 6/10\n",
      "301212/301212 [==============================] - 5s 16us/step - loss: 0.3275 - acc: 0.8338\n",
      "Epoch 7/10\n",
      "301212/301212 [==============================] - 5s 16us/step - loss: 0.3272 - acc: 0.8338\n",
      "Epoch 8/10\n",
      "301212/301212 [==============================] - 5s 16us/step - loss: 0.3267 - acc: 0.8345\n",
      "Epoch 9/10\n",
      "301212/301212 [==============================] - 4s 15us/step - loss: 0.3264 - acc: 0.8351\n",
      "Epoch 10/10\n",
      "301212/301212 [==============================] - 5s 16us/step - loss: 0.3260 - acc: 0.8352\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11a840860>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "# epochs: a fixed number of iterations through the dataset\n",
    "# batch size: the number of instances that are evaluated before a weight update in the network is performed \n",
    "model.fit(X_train, Y_train, epochs = 10, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75304/75304 [==============================] - 2s 21us/step\n",
      "\n",
      "acc: 83.58%\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "scores = model.evaluate(X_test, Y_test)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the prediction bw. 0 and 1\n",
      "[[ 0.55178559]\n",
      " [ 1.        ]\n",
      " [ 0.58622521]\n",
      " ..., \n",
      " [ 1.        ]\n",
      " [ 1.        ]\n",
      " [ 1.        ]]\n",
      "the rounded prediction\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "# calculate predictions\n",
    "predictions = model.predict(X)\n",
    "print(\"the prediction bw. 0 and 1\")\n",
    "print(predictions)\n",
    "\n",
    "# round predictions\n",
    "rounded = [round(x[0]) for x in predictions]\n",
    "print(\"the rounded prediction\")\n",
    "print(rounded[0:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision=0.8129603280663558, recall=0.8097893105578868, fscore=0.811331497783764, support=None\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "precision, recall, fscore, support = score(Y_test, [round(x[0]) for x in model.predict(X_test)],average=\"macro\")\n",
    "print(\"precision={}, recall={}, fscore={}, support={}\".format(precision, recall, fscore, support))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Save Trained Model\n",
    "Save the trained model to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    " \n",
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"models/model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"models/model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Load Saved Model\n",
    "Load the saved model, and used it. It saves training time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n",
      "Use loaded model to predict\n",
      "the prediction bw. 0 and 1\n",
      "[[ 0.55178559]\n",
      " [ 1.        ]\n",
      " [ 0.58622521]\n",
      " ..., \n",
      " [ 1.        ]\n",
      " [ 1.        ]\n",
      " [ 1.        ]]\n",
      "the rounded prediction\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "# load json and create model\n",
    "json_file = open('models/model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"models/model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    " \n",
    "print(\"Use loaded model to predict\")\n",
    "# evaluate loaded model on test data\n",
    "loaded_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# calculate predictions\n",
    "predictions = model.predict(X)\n",
    "print(\"the prediction bw. 0 and 1\")\n",
    "print(predictions)\n",
    "\n",
    "# round predictions\n",
    "rounded = [round(x[0]) for x in predictions]\n",
    "print(\"the rounded prediction\")\n",
    "print(rounded[0:10])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From saved model,  acc: 83.58%\n"
     ]
    }
   ],
   "source": [
    "score = loaded_model.evaluate(X_test, Y_test, verbose=0)\n",
    "print(\"From saved model,  %s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision=0.8129603280663558, recall=0.8097893105578868, fscore=0.811331497783764, support=None\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "precision, recall, fscore, support = score(Y_test, [round(x[0]) for x in model.predict(X_test)],average=\"macro\")\n",
    "print(\"precision={}, recall={}, fscore={}, support={}\".format(precision, recall, fscore, support))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
